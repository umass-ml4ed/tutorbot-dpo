{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing More Dependencies\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import os\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBase(Dataset):\n",
    "    def __getitem__(self, index: int):\n",
    "        # return self.data[index]\n",
    "        item = self.data[index]\n",
    "        # Explicitly return only the required fields\n",
    "        return {\n",
    "            \"text\": item[\"text\"],\n",
    "            \"label\": item[\"label\"]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(DatasetBase):\n",
    "    def __init__(self, data: List[dict], tokenizer: AutoTokenizer):\n",
    "        system_prompt = \"You are an math AI tutor respond to a student based on the conversation history to solve a math question.\"\n",
    "        self.data = [\n",
    "            {\n",
    "                \"text\": tokenizer.apply_chat_template([\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Math problem: {sample['question']}\\nStudent incorrect solution: {sample['student_incorrect_solution']}\\nConversation history: {sample['conversation']}\"}\n",
    "                ], tokenize=False),\n",
    "                \"label\": sample[\"teacher_responses\"]\n",
    "            }\n",
    "            for sample in data\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __init__(self, tokenizer: AutoTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "    \n",
    "        all_prompts = [sample[\"text\"] for sample in batch]\n",
    "        prompts_tokenized = self.tokenizer(all_prompts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        all_inputs = [sample[\"text\"] + sample[\"label\"] + self.tokenizer.eos_token for sample in batch]\n",
    "        inputs_tokenized = self.tokenizer(all_inputs, return_tensors=\"pt\", padding=True)\n",
    "        prompt_lens = prompts_tokenized.attention_mask.sum(dim=1)\n",
    "        labels = inputs_tokenized.input_ids.clone()\n",
    "        padding_mask = torch.arange(labels.shape[1]).repeat(labels.shape[0], 1) < prompt_lens.unsqueeze(1)\n",
    "        labels[padding_mask] = -100\n",
    "        labels = labels.masked_fill(inputs_tokenized.attention_mask == 0, -100)\n",
    "        return {\n",
    "            \"input_ids\": inputs_tokenized.input_ids,\n",
    "            \"attention_mask\": inputs_tokenized.attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for idx, row in data[:500].iterrows():\n",
    "    \n",
    "    for idx, conversation in enumerate(row[\"format_conversation_teacher\"]): \n",
    "        train_data.append({\n",
    "            \"question\": row[\"question\"],\n",
    "            \"student_incorrect_solution\": row[\"student_incorrect_solution\"],\n",
    "            \"conversation\": conversation,\n",
    "            \"teacher_responses\": row[\"teacher_responses\"][idx]\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = train_data[int(len(train_data) * 0.9):]\n",
    "train_data = train_data[:int(len(train_data) * 0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "        output_dir=\"sft_test\",              ### output directory\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=16,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=1e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        max_steps=200,\n",
    "        fp16=True,\n",
    "        push_to_hub=False,\n",
    "        max_seq_length=1024,\n",
    "        remove_unused_columns=False,        ### need to include for including \"text\"\n",
    "        label_names=[\"labels\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7ee6ad3b0e4535af44e4ac01f21192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "  )\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id, quantization_config=bnb_config, device_map=\"auto\", \n",
    "  )\n",
    "  model.config.use_cache=False\n",
    "  model.config.pretraining_tp=1\n",
    "  return model, tokenizer\n",
    "\n",
    "model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3167248/3336414127.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=ConversationDataset(train_data, tokenizer),\n",
    "        eval_dataset=ConversationDataset(test_data, tokenizer),\n",
    "        data_collator=DataCollator(tokenizer),\n",
    "        peft_config=peft_config,\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 1:01:44, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.842800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.614500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.314100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model at the end of training\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
